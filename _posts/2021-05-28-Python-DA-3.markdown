---
layout: post
title:  "플레이 스토어 리뷰 크롤링 - 3"
date:   2021-05-28 21:45:00 +0900
image:  python_da_1_frontimg.PNG
tags:   플레이스토어 크롤링 파이썬 셀레니움
---

### 2개월만의 업데이트

분명 2번째 게시물에서 앱 링크를 하나씩 불러와 여는 기능을 올린다 했는데...그로부터 2개월이 지났다.

물론 2개월이란 시간동안 아무런 진전이 없었던 것은 아니지만 그 진전을 공유할 여유가 없었음을 양해 부탁드린다.

일단 직전 포스트의 내용을 이어 설명하자면, 파일로 저장한 각 앱 링크를 하나씩 여는 코드를 만들었다. 이후 개별 앱 링크를 받아 웹페이지를 열고 크롤링을 진행하는 함수, 그리고 크롤링 함수에서 만들어진 데이터를 엑셀로 저장하는 함수, 마지막으로 위 핵심 함수들을 하나로 묶어 실행시키는 함수까지 완성하였다.

***

![]({{site.baseurl}}/images/python_da_3_img1.PNG)
*앱 링크 txt파일을 불러와 링크를 순차적으로 여는 함수*

![]({{site.baseurl}}/images/python_da_img1.PNG)
*실질적 크롤링을 하는 함수, 앱 이름, 개발사 이름 등 기본적인 정보부터 전체 리뷰를 보는 기능까지 구현되어있다.*

![]({{site.baseurl}}/images/python_da_img1.PNG)
*위 크롤링 함수의 데이터를 받아 엑셀에 저장하는 함수*

![]({{site.baseurl}}/images/python_da_img6.PNG)
*크롤링에 필요한 모든 함수를 총집합시킨 함수*

참고로 위 사진의 코드들은 2개월동안 이런저런 수정이 이루어진 코드들이다.

***

위 코드를 작성 및 실행하며 느낀점은 크게 4가지이다.

1. 크롤링 자체는 컴퓨터 자원을 크게 소모하지 않는다.
  - 셀레니움 사용으로 멀티프로세싱 사용이 불가능해 유의미한 차이를 얻을 수 없었다. 만약 셀레니움에 멀티프로세싱 기능을 사용할 수 있는 방법을 아신다면 꼭 연락 부탁드린다.
2. 데이터를 순차적으로 정리 및 재구성 하는 것에는 큰 시간이 걸린다.
3. 웹페이지를 실제로 띄우는 것보다 셀레니움의 headless 옵션을 사용하는 것이 훨씬 컴퓨터 자원을 적게 사용한다.
4. 각 단계별로 print를 사용해 시간을 나타내는 것이 이후 디버깅에 큰 도움이 된다.

먼저, 본인의 컴퓨터는 그리 고사양이 아니다. 

현재 i5-6500, 16GB RAM, GTX 1060 3GB로 구성된 시스템을 사용중인데, 크롤링을 하는 과정에서 작업관리자상의 성늠탭에서 평소와 큰 차이를 보여주지 않았고 동시에 전체적인 성능저하가 없었기에 크롤링 자체는 컴퓨터 자원을 크게 소모하지 않는다고 결론내렸다.

하지만 크롤링된 데이터를 순차적으로 정리 및 재구성(짧은 리뷰, 긴 리뷰를 순서에 맞게 정리 하는 것)하는 작업과 여러 데이터를 하나로 묶는 작업은 큰 시간이 걸렸다. 일례로 케이크 라는 어플의 리뷰 약 19000개의 크롤링은 약 한시간이 걸렸지만, 엑셀에 데이터를 저장할때까지 9시간정도 걸렸다. 아무래도 작업이 순차적으로 이루어져서 그렇다고 생각된다.

크롤링과정에서 웹페이지가 제대로 보여지지 않아 리뷰 크롤링이 중간에 중단되고 다음 단계(데이터 저장 단계)로 넘어가는 경우가 많았다. 이는 한 웹페이지에 끊임없는 스크롤 다운이 지속되고 표시할 요소가 많아짐에 따라 느려짐이 발생해 일어나는 일이라 생각되었고 이를 해결하기 위해 Webdriver에 headless옵션을 적용하였다. 이후 이전과 동일한 문제는 크게 발생하지 않았다.

마지막으로 한 작업에서 다른 작업으로 넘어갈때 print에 datetime을 이용해 시간을 표기하는 것이 디버깅에 큰 도움이 되었다. 이 경우 어느 부분이 느린지, 어느 부분에서 오류가 발생했는지 등 많은 정보를 유추 할 수 있게 해주었고 완벽히 해결된 부분이 아니라면 꼬리표를 달듯 에러 트래킹이 가능하도록 해주었다. 아주 유용한 부분이었다.



*** 

일부 코드는 본인이 작성한 것이 아님을 먼저 밝힌다.

***

코드는 교수님께서 제공해주신 코드를 입맛에 맞게 수정 및 조합한 뒤 필요한 부분을 추가로 작성하는 방식으로 작성했다.

이때 함수로 선언되지 않고 전역에서 실행되던 코드들을 기능별 함수로 묶었고, 

코드의 어느 부분을 수정해야 하는지 잘 모르는 분들을 위해 개인에 맞춰 자주 변경해야 하는 부분(인스타 그램 id, pw, 검색 키워드등)은 input으로 처리하여 실행할 때마다 변경이 가능하도록 하였으며

함수마다 달리 저장되는 엑셀 파일의 이름을 기억하기에 어려움이 있을 것 같아

모든 함수의 실행에는 크롤링시 사용한 키워드(ex: 남이섬맛집) 만을 입력하도록 처리했다.

동시에 진행 경과를 알 수 있게 tdqm을 이용하였고 자잘한 에러를 수정하였다.

~~사실 더 많이 바꾼 것 같지만 워낙 많아 이만 줄인다.~~

***

마지막으로, 구글에 인스타그램 크롤링이라 검색했을때 제시되는 코드와 동일한 코드도 있는데 이 부분은 본인도 각종 피드백에 대응하며 알게 되었다. 

이는 강의에 제공된 코드의 일부와 인터넷에 제공된 코드의 일부 동일한 코드가 존재했을 뿐이지 본인이 의도적으로 다른 분들이 공유하신 코드를 단순 짜집기 하여 '모두 내 손으로 일궜다!' 라고 주장하려는 것이 아님을 알아주시기 바란다.

***

공유하려는 코드에는 크게 3가지 기능이 포함되어있다.

1. 인스타그램 게시글 크롤링(본문, 날짜, 종아요, 위치, 해시태그)
2. 지도 좌표 크롤링(카카오 API 이용)
3. 해시태그를 이용한 워드클라우드 생성

동시에 위 기능들 중 대부분은 데이터를 엑셀로 내보내고 있음을 알린다.

셋 중 필요한 기능이 있다면 아래 제공된 코드를 사용하시면 될 것 같다.

#### 코드 사용방법
***

1. 하단의 링크를 클릭하면 구글 드라이브 (공개용) 폴더로 연결된다.
2. (권장) 해당 폴더를 통째로 다운로드 하여 압축을 푼다.
- (비권장) 해당 폴더의 냠이섬_프로젝트_코드.ipnyb 와 chromedriver.exe를 다운로드 한 뒤 한 폴더에 넣고 해당 폴더 안에 files 라는 이름의 폴더를 생성한다.
3. 주피터노트북 상에서 냠이섬_프로젝트_코드.ipnyb 를 연다.
4. 해당 노트북에 작성된 사용 설명을 읽고 실행한다.

***

[코드 보러가기](https://drive.google.com/drive/folders/1aRUHS5Be6yJe2AhWRsMCMQ1XwdKwdVsu?usp=sharing)

다운로드를 받지 않고 코드만 읽고 싶다면 구글 콜랩 사용을 추천한다.

크롤링 부분의 코드가 콜랩에 맞게 수정되지 않아 작동하진 않겠지만 그저 코드를 훑어 보는 것에는 큰 문제가 없을 것이라 생각된다.

만약 문제가 발생한다면 코드 내 기재된 이메일 또는 StrangeFate.K@gmail.com 으로 연락주시고

도움이 되어 이 게시글 링크를 출처로 남겨주시겠다면 마다하지 않겠다.
